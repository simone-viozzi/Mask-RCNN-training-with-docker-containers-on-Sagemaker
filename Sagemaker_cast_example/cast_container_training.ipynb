{
 "cells": [
  {
   "source": [
    "## Sagemaker container lesion example\n",
    "In this example is launched a dummy container using spot instances, to test what's the behavior of the aws Sagemaker with docker containers, and what's the behaviour of the container whene it is terminated by aws due to lack of spot resources.</p>\n",
    "<p>In this example is simulated by a dummy python script (into the container) that performs similar actions that a normal training script with tensorflow or other framework should do, more specificaly:\n",
    "\n",
    "- Fake checkpoints are written in txt format in the folder `/opt/ml/checkpoints/`.\n",
    "\n",
    "- Fake tensorboard records are written every 20 seconds into the folder `/opt/ml/output/tensorboard/`, for check the real-time prensence into the s3 bucket folder  specified.\n",
    "\n",
    "- Furthermore the tree command is executed in the path `/opt/ml/` for inspect the folder structure created by sagemaker, and the result is stored in the `/opt/output/data/` folder as a txt file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06426f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.debugger import TensorBoardOutputConfig"
   ]
  },
  {
   "source": [
    "In this section is recovered the Sagemaker bucket generated by default from the service"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf426bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default sagemaker bucket name request \n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "sagemaker_default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(\"bucket generated by sagemacker:\" + sagemaker_default_bucket)"
   ]
  },
  {
   "source": [
    "############################  JOB NAME  ####################################\n",
    "\n",
    "# job-name definition:\n",
    "# every multi-job session of training jobs is characterized by a base-job-name\n",
    "# the job-name on the contrary is the identifier of the single training job.\n",
    "# The job-name must be different for each training job and should be used to\n",
    "# divide the results of different training jobs into specific folders.\n",
    "job_name = 'cast-test-2-slide-windows'"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Defining the s3 bucket for the training job\n",
    "In this section are defined all the nedeed variables that specify the paths to s3 buckets for the inputs and outputs data.\n",
    "\n",
    "It's worth spending a few words about the configuration of the TensorBoardOutputConfig, this path can be used for tensorboard data if you use tensorflow or for other types of files that is important for you to take out of the container during the training process. in this example we write some txt files filled with random chearacters in the `container_local_output_path` and this files became available in the `s3_output_path` relative to the TensorBoardOutputConfig in a few seconds.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fc4dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################   INPUTS  ####################################\n",
    "\n",
    "# repositroy ECR containing the docker image configured to be executed by Sagemaker\n",
    "# ecr_container_uri = \"<your aws id>.dkr.ecr.<your aws region>.amazonaws.com/<your repo name:your repo tag>\"\n",
    "ecr_container_uri = \"011827850615.dkr.ecr.eu-west-1.amazonaws.com/maskrcnn_repo_test:cast_2\"\n",
    "#ecr_container_uri = \"011827850615.dkr.ecr.eu-west-1.amazonaws.com/maskrcnn_repo_test:lesion_2\"\n",
    "\n",
    "# s3 path containing the dataset needed for training the model\n",
    "dataset_bucket = \"s3://datsetsbucket/cast_dataset_slide/\"\n",
    "\n",
    "# s3 path containing the model with pretrained weights, in the next example in this folder would be\n",
    "# stored the Mask R-CNN model trained on COCO. \n",
    "model_bucket = 's3://cermodelbucket'\n",
    "\n",
    "############################  OUTPUTS  ####################################\n",
    "\n",
    "# s3 path where are stored the results of the instance profiler and any other data saved during the training in the folder /opt/ml/output/data/\n",
    "output_path = f's3://{sagemaker_default_bucket}/output'\n",
    "\n",
    "# s3 path where are stored the checkpoints of the training proces\n",
    "checkpoints_path = f'{output_path}/{job_name}/checkpoints'\n",
    "\n",
    "# internal paths for checkpoints and tenorboard logs passed to the container as env variables\n",
    "user_defined_env_vars = {\"checkpoints\": \"/opt/ml/checkpoints\",\n",
    "                        \"tensorboard\": \"/opt/ml/output/tensorboard\"}\n",
    "\n",
    "# Definition of s3 target bucket folder for the tensorboard outputs and container folder where the tensorboard record must to be placed.\n",
    "# it's possible to place the tensorboard output in other places but sagemaker copy that records into '/opt/ml/output/tensorboard' so we decide to\n",
    "# put the records directly in there.\n",
    "# Note: in the path 's3://testtflogs/logs' the recors are divided into folders related to the job-name in this example the output of tensorboard\n",
    "# should be fine in 's3://testtflogs/logs/test-11'\n",
    "tensorboard_output_config = TensorBoardOutputConfig(\n",
    "    s3_output_path='s3://testtflogs/logs',\n",
    "    container_local_output_path=user_defined_env_vars['tensorboard']\n",
    ")\n"
   ]
  },
  {
   "source": [
    "In this section are recovered the execution role ARN associated to this notebook, that will be passed to the estimator for launching the training job, be sure to give permissions to use other buckets to this role, otherwise it will only be possible to use buckets starting with the sagemaker keyword, in this case the permission is needed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b7197",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if you are running the code from jupiter\n",
    "# getting the execution role of this instance of sagemaker notebook\n",
    "# role = get_execution_role()\n",
    "# print(role)\n",
    "\n",
    "# if you are running the code from local\n",
    "role = 'arn:aws:iam::011827850615:role/service-role/AmazonSageMaker-ExecutionRole-20210522T125188'\n",
    "# role = 'arn:aws:iam::<your aws id>:role/service-role/<your role name>'"
   ]
  },
  {
   "source": [
    "Definitions of regex for logs extraction from stdout of the training script in the container"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    {\"Name\": \"loss\",                    \"Regex\": r\"\\sloss:\\s(\\d+.?\\d*)\\s-\"},\n",
    "    {\"Name\": \"rpn_class_loss\",          \"Regex\": r\"\\srpn_class_loss:\\s(\\d+.?\\d*)\\s-\"},\n",
    "    {\"Name\": \"rpn_bbox_loss\",           \"Regex\": r\"\\srpn_bbox_loss:\\s(\\d+.?\\d*)\\s-\"},\n",
    "    {\"Name\": \"mrcnn_class_loss\",        \"Regex\": r\"\\smrcnn_class_loss:\\s(\\d+.?\\d*)\\s-\"},\n",
    "    {\"Name\": \"val_loss\",                \"Regex\": r\"\\smrcnn_bbox_loss:\\s(\\d+.?\\d*)\\s-\"},\n",
    "    {\"Name\": \"mrcnn_mask_loss\",         \"Regex\": r\"\\smrcnn_mask_loss:\\s(\\d+.?\\d*)\\s\"},\n",
    "    {\"Name\": \"mrcnn_bbox_loss\",         \"Regex\": r\"\\sval_loss:\\s(\\d+.?\\d*)\\s-\"},\n",
    "    {\"Name\": \"val_rpn_class_loss\",      \"Regex\": r\"\\sval_rpn_class_loss:\\s(\\d+.?\\d*)\\s-\"},\n",
    "    {\"Name\": \"val_rpn_bbox_loss\",       \"Regex\": r\"\\sval_rpn_bbox_loss:\\s(\\d+.?\\d*)\\s-\"},\n",
    "    {\"Name\": \"val_mrcnn_class_loss\",    \"Regex\": r\"\\sval_mrcnn_class_loss:\\s(\\d+.?\\d*)\\s-\"},\n",
    "    {\"Name\": \"val_mrcnn_bbox_loss\",     \"Regex\": r\"\\sval_mrcnn_bbox_loss:\\s(\\d+.?\\d*)\\s-\"},\n",
    "    {\"Name\": \"val_mrcnn_mask_loss\",     \"Regex\": r\"\\sval_mrcnn_mask_loss:\\s(\\d+.?\\d*)\"},\n",
    "    {\"Name\": \"ms/step\",                 \"Regex\": r\"\\s(\\d+)ms\\/step\"},\n",
    "    {\"Name\": \"epoch\",                   \"Regex\": r\"Epoch\\s(\\d+)\\/\\d*\"}\n",
    "]"
   ]
  },
  {
   "source": [
    "In this section are defined the hyperparameters, this values are passed to the estimator definitions and would be reachable from the trainning script in the container as commandline arguments, or like environment variables whit this notation `SM_HP_{hyperparameter_name}`, es. `SM_HP_HP1` or `SM_HP_BATCH` in this case"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e29ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters definition\n",
    "hyperparameters = {\n",
    "    \"NAME\": \"cast\", \n",
    "    \"GPU_COUNT\": 1, \n",
    "    \"IMAGES_PER_GPU\": 1,\n",
    "    \"AUG_PREST\": 1,\n",
    "    \"TRAIN_SEQ\": \"[\\\n",
    "        {\\\"epochs\\\": 15, \\\"layers\\\": \\\"all\\\", \\\"lr\\\": 0.002 },\\\n",
    "        {\\\"epochs\\\": 30, \\\"layers\\\": \\\"all\\\", \\\"lr\\\": 0.0002 }\\\n",
    "    ]\"\n",
    "}"
   ]
  },
  {
   "source": [
    "## Setup the training job\n",
    "This is the key function of the script, in there are configured al the training job parameters, are passed all the path that was defined earlier, the hyperparameters and are defined many settings relative to the type of machine used for the job, and in witch mode should run.\n",
    "\n",
    "More specificaly we chose to run in spot mode (`use_spot_instances = True`), in this mode the cost of the training goes down from 50% to 80% depending on the instance type chosen and by the availability of the machine, this mode enable aws to sell at lower price unused compute capability in the cloud and can stop your application if someone need this machine in on-demand mode (without any discount).\n",
    "\n",
    "Whene you chose to run in spot mode two more variables should be set, `max_run` and `max_wait`, this variables specify how match time in seconds the container could run and the second specify in the case that it will be stoped by aws how much time the program should wait that spot instances became available again for restart your container, if one of the two limits are exceeded the training job will be terminated.\n",
    "\n",
    "For other info about the parameters of this function you can check the [reference](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaf3f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_test = sagemaker.estimator.Estimator(\n",
    "    # container image \n",
    "    image_uri    = ecr_container_uri, \n",
    "    # role of sagemaker notebook instance\n",
    "    role         = role, \n",
    "    # number of instance to launch\n",
    "    instance_count = 1, \n",
    "    # if you want to use local mode\n",
    "    #train_instance_type=\"local\",  \n",
    "    # type of instance in which place the training job\n",
    "    # instance_type = 'ml.g4dn.xlarge',\n",
    "    instance_type = 'ml.g4dn.2xlarge',\n",
    "    # instance_type = 'ml.g4dn.12xlarge',\n",
    "    # instance_type = 'ml.p3.2xlarge',\n",
    "    # space in GB of the storage attached to the ec2 instance\n",
    "    #volume_size  = 50,\n",
    "    # max number of seconds of running for the job until the termination of the process\n",
    "    max_run      = 36*3600,\n",
    "    # s3 path where will be placed the results of the profiler and the content of /opt/ml/output/data/ path as tar.gz file \n",
    "    output_path  = output_path, \n",
    "    # prefix for the trainng job name, if not specified generated automatically\n",
    "    # base_job_name=\"training-test\", \n",
    "    # training job hyperparameters, parameters passed as command line arguments\n",
    "    hyperparameters = hyperparameters,\n",
    "    # list of tags for the job\n",
    "    tags = [{\"Key\": \"CER\", \"Value\": \"1\"},],\n",
    "    # s3 path where to find the data needed for the start of the training job like the pretrained model,and that will be copied into the folder /opt/ml/inputs/data/model (sovrascrivibile da model_channel_name)\n",
    "    model_uri    = model_bucket, \n",
    "    # name of the chanel where will be saved the data included in the path model_uri (/opt/ml/inputs/data/<model_channel_name>)\n",
    "    model_channel_name = 'model', \n",
    "    # dict regexs for metrics extraction from stdout {\"<metric name>\":\"<regex for logs estraction>\", ...}\n",
    "    metric_definitions = metrics, \n",
    "    # flag for enabling the spot training\n",
    "    use_spot_instances = True, \n",
    "    # max time of waiting for spot instance to became available again\n",
    "    max_wait = 48*3600, \n",
    "    # s3 chekpoints target path\n",
    "    checkpoint_s3_uri = checkpoints_path, \n",
    "    # default: '/opt/ml/checkpoints'\n",
    "    checkpoint_local_path = user_defined_env_vars['checkpoints'], \n",
    "    # SageMaker Debugger rules\n",
    "    #rules = ; \n",
    "    # Tensorboard output configuration\n",
    "    tensorboard_output_config = tensorboard_output_config,\n",
    "    # dict usefull for setting more environment variables into the container\n",
    "    environment = user_defined_env_vars,\n",
    "    # max number of try to restart the job if it's finish unespectedly\n",
    "    #max_retry_attempts =   \n",
    ")"
   ]
  },
  {
   "source": [
    "## Launching the training job\n",
    "In this section of the notebook the training-job start, using the `.fit()` method of estimator object.\n",
    "\n",
    "This method have the input parameter that could be defined as a dict with a key and a path to local storage or s3 bucket where are present files that we would download to the container, note that the number of path that could be specified are not limited and for each input path in the container into `/opt/ml/inputs/` is created one folder with the name equal to the passed key name and containing the data into the argument path. In this case we only passed one path to input param and we have as a result `/opt/ml/inputs/dataset/` into the container filled with the same data placed into `dataset_bucket ='s3://datsetsbucket/isic2018/test_dataset/'`, if you pass this dict with more values the result will be an `inputs/` folder populated with more subfolders and relative data.\n",
    "\n",
    "The `job_name` is a very important parameter this enable you to distinguish from different jobs launched simultaniously or in different moments and permit to distinguish the training job in the training job panel for this reason they can't have the same name, if you launch two training job with the same name the result is an error and the container dosen't start.\n",
    "\n",
    "Another note is relative to wait and logs parameters, this parameters enable you to watch the logs relative to the startup of the machine and to watch the training job machine logs outputs at the end of the training process (it's not possible to see the training job stdout in real-time). With this configuration the function fit don't terminate until the job isn't finished, but if you don't enable the wait parameter the job start and fit function terminate so you can launch other functions or other jobs,  so to see the logs the wait param should be true.\n",
    "\n",
    "If something is unclear you can check the relative [reference](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24081b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_test.fit(\n",
    "    inputs      = {\n",
    "        'dataset': dataset_bucket\n",
    "    },\n",
    "    job_name    = job_name,\n",
    "    wait        = True,\n",
    "    logs        = 'All'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python395jvsc74a57bd0767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90",
   "display_name": "Python 3.9.5 64-bit ('usr')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}